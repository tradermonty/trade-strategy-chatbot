"""
ğŸŸ¢ Green ãƒ•ã‚§ãƒ¼ã‚º: ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®æœ€å°é™ã®å®Ÿè£…
ETLå‡¦ç†ã§Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«å¤‰æ›
ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°æ©Ÿèƒ½ã‚’å«ã‚€
"""

import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from config import Config
from langchain.text_splitter import MarkdownTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

class KnowledgeIngester:
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã®å–ã‚Šè¾¼ã¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ã‚¤ãƒ‹ã‚·ãƒ£ãƒ©ã‚¤ã‚¶ãƒ¼ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®æœ€å°é™ã®å®Ÿè£…"""
        self.knowledge_path = Config.KNOWLEDGE_PATH
        self.vector_store_path = Config.VECTOR_STORE_PATH
        self.text_splitter = MarkdownTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    def load_markdown_files(self) -> List[str]:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        knowledge_dir = Path(self.knowledge_path)
        markdown_files = []
        
        if knowledge_dir.exists():
            for md_file in knowledge_dir.glob("*.md"):
                markdown_files.append(str(md_file))
        
        return markdown_files
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰² - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        chunks = self.text_splitter.split_text(text)
        return chunks
    
    def create_documents_from_chunks(self, chunks: List[str], source_file: str) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                "page_content": chunk,
                "metadata": {
                    "source": source_file,
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                }
            }
            documents.append(doc)
        return documents
    
    def create_vector_store(self, documents: List[Dict[str, Any]]):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        # LangChainã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from langchain.schema import Document
        
        langchain_docs = []
        for doc in documents:
            langchain_doc = Document(
                page_content=doc["page_content"],
                metadata=doc["metadata"]
            )
            langchain_docs.append(langchain_doc)
        
        # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vector_store = FAISS.from_documents(
            langchain_docs,
            self.embeddings
        )
        
        return vector_store
    
    def save_vector_store(self, vector_store):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        save_path = Path(self.vector_store_path)
        save_path.mkdir(exist_ok=True)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        vector_store.save_local(str(save_path))
        
        return str(save_path)
    
    def run(self) -> List[Dict[str, Any]]:
        """ETLå‡¦ç†ã®çµ±åˆå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰"""
        print("ğŸ“š Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")
        
        # 1. Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        markdown_files = self.load_markdown_files()
        if not markdown_files:
            print("âŒ Markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        print(f"ğŸ“ {len(markdown_files)}å€‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
        
        all_documents = []
        
        # 2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for file_path in markdown_files:
            print(f"ğŸ“„ å‡¦ç†ä¸­: {Path(file_path).name}")
            
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
                chunks = self.split_text_into_chunks(content)
                print(f"  ğŸ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
                documents = self.create_documents_from_chunks(chunks, file_path)
                all_documents.extend(documents)
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not all_documents:
            print("âŒ å‡¦ç†å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        print(f"ğŸ“Š åˆè¨ˆ {len(all_documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        
        # 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
        print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        try:
            vector_store = self.create_vector_store(all_documents)
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            
            # 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜
            print("ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜ä¸­...")
            save_path = self.save_vector_store(vector_store)
            print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            
            return all_documents
            
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []


class IncrementalIngester(KnowledgeIngester):
    """ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹Ingesterã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        super().__init__()
        self.metadata_file = Path(self.vector_store_path) / "file_metadata.json"
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash
    
    def _get_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        file_stat = os.stat(file_path)
        return {
            "path": file_path,
            "size": file_stat.st_size,
            "mtime": file_stat.st_mtime,
            "hash": self._calculate_file_hash(file_path),
            "last_processed": datetime.now().isoformat()
        }
    
    def _load_metadata(self) -> Dict[str, Dict[str, Any]]:
        """ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_metadata(self, metadata: Dict[str, Dict[str, Any]]):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        self.metadata_file.parent.mkdir(exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _file_has_changed(self, file_path: str, stored_metadata: Dict[str, Any]) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯"""
        if file_path not in stored_metadata:
            return True
        
        current_meta = self._get_file_metadata(file_path)
        stored_meta = stored_metadata[file_path]
        
        # ãƒãƒƒã‚·ãƒ¥å€¤ã§å¤‰æ›´ã‚’æ¤œçŸ¥
        return current_meta["hash"] != stored_meta.get("hash", "")
    
    def _generate_document_id(self, file_path: str, chunk_id: int) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
        file_name = Path(file_path).name
        return f"{file_name}::{chunk_id}"
    
    def _load_existing_vector_store(self) -> Optional[FAISS]:
        """æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿"""
        vector_store_path = Path(self.vector_store_path)
        if vector_store_path.exists():
            try:
                return FAISS.load_local(
                    str(vector_store_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                print(f"âš ï¸  æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return None
        return None
    
    def add_knowledge_file(self, file_path: str) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ """
        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ : {Path(file_path).name}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
            chunks = self.split_text_into_chunks(content)
            print(f"  ğŸ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": file_path,
                        "chunk_id": i,
                        "total_chunks": len(chunks),
                        "document_id": self._generate_document_id(file_path, i)
                    }
                )
                documents.append(doc)
            
            # æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿
            vector_store = self._load_existing_vector_store()
            
            if vector_store is None:
                # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
                print("  ğŸ†• æ–°è¦ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ")
                vector_store = FAISS.from_documents(documents, self.embeddings)
            else:
                # æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
                print("  â• æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ ")
                vector_store.add_documents(documents)
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜
            self.save_vector_store(vector_store)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            metadata = self._load_metadata()
            metadata[file_path] = self._get_file_metadata(file_path)
            self._save_metadata(metadata)
            
            print(f"  âœ… ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ å®Œäº†: {len(documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
            return True
            
        except Exception as e:
            print(f"  âŒ ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def remove_knowledge_file(self, file_path: str) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‰Šé™¤"""
        print(f"ğŸ—‘ï¸  ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {Path(file_path).name}")
        
        try:
            # æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿
            vector_store = self._load_existing_vector_store()
            if vector_store is None:
                print("  âš ï¸  ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            # å‰Šé™¤å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’ç‰¹å®š
            # æ³¨æ„: FAISSã¯ç›´æ¥çš„ãªå‰Šé™¤æ©Ÿèƒ½ãŒé™å®šçš„ãªãŸã‚ã€
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯å…¨ä½“ã‚’å†æ§‹ç¯‰ã™ã‚‹æ–¹ãŒç¢ºå®Ÿ
            print("  âš ï¸  FAISSåˆ¶é™ã®ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«ã¯å…¨ä½“å†æ§‹ç¯‰ã‚’æ¨å¥¨")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤
            metadata = self._load_metadata()
            if file_path in metadata:
                del metadata[file_path]
                self._save_metadata(metadata)
                print("  âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤å®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"  âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def update_knowledge_file(self, file_path: str) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼ˆå‰Šé™¤â†’è¿½åŠ ï¼‰"""
        print(f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {Path(file_path).name}")
        
        # ã¾ãšå‰Šé™¤ã—ã¦ã‹ã‚‰è¿½åŠ 
        self.remove_knowledge_file(file_path)
        return self.add_knowledge_file(file_path)
    
    def incremental_update(self) -> Dict[str, int]:
        """å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’æ›´æ–°"""
        print("ğŸ”„ ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°é–‹å§‹...")
        
        # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        current_files = self.load_markdown_files()
        stored_metadata = self._load_metadata()
        
        stats = {
            "added": 0,
            "updated": 0,
            "removed": 0,
            "unchanged": 0
        }
        
        # æ–°è¦ãƒ»æ›´æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        for file_path in current_files:
            if self._file_has_changed(file_path, stored_metadata):
                if file_path in stored_metadata:
                    # æ›´æ–°
                    if self.update_knowledge_file(file_path):
                        stats["updated"] += 1
                else:
                    # æ–°è¦è¿½åŠ 
                    if self.add_knowledge_file(file_path):
                        stats["added"] += 1
            else:
                stats["unchanged"] += 1
        
        # å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        current_file_set = set(current_files)
        for stored_file in stored_metadata.keys():
            if stored_file not in current_file_set:
                if self.remove_knowledge_file(stored_file):
                    stats["removed"] += 1
        
        print(f"ğŸ“Š ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«æ›´æ–°å®Œäº†:")
        print(f"  â• è¿½åŠ : {stats['added']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"  ğŸ”„ æ›´æ–°: {stats['updated']}ãƒ•ã‚¡ã‚¤ãƒ«")  
        print(f"  ğŸ—‘ï¸  å‰Šé™¤: {stats['removed']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"  âœ… å¤‰æ›´ãªã—: {stats['unchanged']}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        return stats


if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ãƒ†ã‚¹ãƒˆ
    ingester = KnowledgeIngester()
    documents = ingester.run()
    print(f"å‡¦ç†å®Œäº†: {len(documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ") 