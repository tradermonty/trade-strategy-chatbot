"""
ğŸŸ¢ Green ãƒ•ã‚§ãƒ¼ã‚º: ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®æœ€å°é™ã®å®Ÿè£…
ETLå‡¦ç†ã§Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«å¤‰æ›
"""

from pathlib import Path
from typing import List, Dict, Any
from config import Config
from langchain.text_splitter import MarkdownTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

class KnowledgeIngester:
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã®å–ã‚Šè¾¼ã¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ã‚¤ãƒ‹ã‚·ãƒ£ãƒ©ã‚¤ã‚¶ãƒ¼ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®æœ€å°é™ã®å®Ÿè£…"""
        self.knowledge_path = Config.KNOWLEDGE_PATH
        self.vector_store_path = Config.VECTOR_STORE_PATH
        self.text_splitter = MarkdownTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    def load_markdown_files(self) -> List[str]:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        knowledge_dir = Path(self.knowledge_path)
        markdown_files = []
        
        if knowledge_dir.exists():
            for md_file in knowledge_dir.glob("*.md"):
                markdown_files.append(str(md_file))
        
        return markdown_files
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰² - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        chunks = self.text_splitter.split_text(text)
        return chunks
    
    def create_documents_from_chunks(self, chunks: List[str], source_file: str) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                "page_content": chunk,
                "metadata": {
                    "source": source_file,
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                }
            }
            documents.append(doc)
        return documents
    
    def create_vector_store(self, documents: List[Dict[str, Any]]):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        # LangChainã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from langchain.schema import Document
        
        langchain_docs = []
        for doc in documents:
            langchain_doc = Document(
                page_content=doc["page_content"],
                metadata=doc["metadata"]
            )
            langchain_docs.append(langchain_doc)
        
        # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        vector_store = FAISS.from_documents(
            langchain_docs,
            self.embeddings
        )
        
        return vector_store
    
    def save_vector_store(self, vector_store):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜ - ãƒ†ã‚¹ãƒˆã‚’é€šã™ãŸã‚ã®å®Ÿè£…"""
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        save_path = Path(self.vector_store_path)
        save_path.mkdir(exist_ok=True)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        vector_store.save_local(str(save_path))
        
        return str(save_path)
    
    def run(self) -> List[Dict[str, Any]]:
        """ETLå‡¦ç†ã®çµ±åˆå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰"""
        print("ğŸ“š Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")
        
        # 1. Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        markdown_files = self.load_markdown_files()
        if not markdown_files:
            print("âŒ Markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        print(f"ğŸ“ {len(markdown_files)}å€‹ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
        
        all_documents = []
        
        # 2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for file_path in markdown_files:
            print(f"ğŸ“„ å‡¦ç†ä¸­: {Path(file_path).name}")
            
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
                chunks = self.split_text_into_chunks(content)
                print(f"  ğŸ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
                documents = self.create_documents_from_chunks(chunks, file_path)
                all_documents.extend(documents)
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not all_documents:
            print("âŒ å‡¦ç†å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        print(f"ğŸ“Š åˆè¨ˆ {len(all_documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
        
        # 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆ
        print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        try:
            vector_store = self.create_vector_store(all_documents)
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            
            # 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜
            print("ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜ä¸­...")
            save_path = self.save_vector_store(vector_store)
            print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            
            return all_documents
            
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []


if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ãƒ†ã‚¹ãƒˆ
    ingester = KnowledgeIngester()
    documents = ingester.run()
    print(f"å‡¦ç†å®Œäº†: {len(documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ") 