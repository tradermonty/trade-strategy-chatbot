"""
ğŸŸ¢ Green Phase: Minimal implementation to pass tests
ETL processing to convert Markdown files to vector store
Including incremental update functionality
"""

import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from config import Config
from langchain.text_splitter import MarkdownTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

class KnowledgeIngester:
    """Class for ingesting knowledge files and creating vector store"""
    
    def __init__(self):
        """Initializer - minimal implementation to pass tests"""
        self.knowledge_path = Config.KNOWLEDGE_PATH
        self.vector_store_path = Config.VECTOR_STORE_PATH
        self.text_splitter = MarkdownTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    def load_markdown_files(self) -> List[str]:
        """Markdown file loading - implementation to pass tests"""
        knowledge_dir = Path(self.knowledge_path)
        markdown_files = []
        
        if knowledge_dir.exists():
            for md_file in knowledge_dir.glob("*.md"):
                markdown_files.append(str(md_file))
        
        return markdown_files
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """Text splitting - implementation to pass tests"""
        chunks = self.text_splitter.split_text(text)
        return chunks
    
    def create_documents_from_chunks(self, chunks: List[str], source_file: str) -> List[Dict[str, Any]]:
        """Document creation from chunks - implementation to pass tests"""
        documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                "page_content": chunk,
                "metadata": {
                    "source": source_file,
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                }
            }
            documents.append(doc)
        return documents
    
    def create_vector_store(self, documents: List[Dict[str, Any]]):
        """Vector store creation - implementation to pass tests"""
        # Convert to LangChain Document objects
        from langchain.schema import Document
        
        langchain_docs = []
        for doc in documents:
            langchain_doc = Document(
                page_content=doc["page_content"],
                metadata=doc["metadata"]
            )
            langchain_docs.append(langchain_doc)
        
        # Create FAISS vector store
        vector_store = FAISS.from_documents(
            langchain_docs,
            self.embeddings
        )
        
        return vector_store
    
    def save_vector_store(self, vector_store):
        """Vector store saving - implementation to pass tests"""
        # Create save directory
        save_path = Path(self.vector_store_path)
        save_path.mkdir(exist_ok=True)
        
        # Save FAISS index
        vector_store.save_local(str(save_path))
        
        return str(save_path)
    
    def run(self) -> List[Dict[str, Any]]:
        """Integrated ETL processing method"""
        print("ğŸ“š Searching for Markdown files...")
        
        # 1. Load Markdown files
        markdown_files = self.load_markdown_files()
        if not markdown_files:
            print("âŒ No Markdown files found.")
            return []
        
        print(f"ğŸ“ Found {len(markdown_files)} Markdown files.")
        
        all_documents = []
        
        # 2. Process each file
        for file_path in markdown_files:
            print(f"ğŸ“„ Processing: {Path(file_path).name}")
            
            try:
                # File loading
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Text splitting
                chunks = self.split_text_into_chunks(content)
                print(f"  ğŸ“ Split into {len(chunks)} chunks.")
                
                # Document creation
                documents = self.create_documents_from_chunks(chunks, file_path)
                all_documents.extend(documents)
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not all_documents:
            print("âŒ No processable documents found.")
            return []
        
        print(f"ğŸ“Š Created {len(all_documents)} documents in total.")
        
        # 3. Vector store creation
        print("ğŸ”„ Creating vector store...")
        try:
            vector_store = self.create_vector_store(all_documents)
            print("âœ… Vector store creation completed.")
            
            # 4. Vector store saving
            print("ğŸ’¾ Saving vector store...")
            save_path = self.save_vector_store(vector_store)
            print(f"âœ… Saved vector store to '{save_path}'.")
            
            return all_documents
            
        except Exception as e:
            print(f"âŒ Vector store creation error: {e}")
            return []


class IncrementalIngester(KnowledgeIngester):
    """Ingester class providing incremental update functionality"""
    
    def __init__(self):
        super().__init__()
        self.metadata_file = Path(self.vector_store_path) / "file_metadata.json"
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """Calculate file hash value"""
        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash
    
    def _get_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """Get file metadata"""
        file_stat = os.stat(file_path)
        return {
            "path": file_path,
            "size": file_stat.st_size,
            "mtime": file_stat.st_mtime,
            "hash": self._calculate_file_hash(file_path),
            "last_processed": datetime.now().isoformat()
        }
    
    def _load_metadata(self) -> Dict[str, Dict[str, Any]]:
        """Load saved file metadata"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_metadata(self, metadata: Dict[str, Dict[str, Any]]):
        """Save file metadata"""
        self.metadata_file.parent.mkdir(exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _file_has_changed(self, file_path: str, stored_metadata: Dict[str, Any]) -> bool:
        """Check if file has changed"""
        if file_path not in stored_metadata:
            return True
        
        current_meta = self._get_file_metadata(file_path)
        stored_meta = stored_metadata[file_path]
        
        # Detect changes with hash value
        return current_meta["hash"] != stored_meta.get("hash", "")
    
    def _generate_document_id(self, file_path: str, chunk_id: int) -> str:
        """Generate unique document ID"""
        file_name = Path(file_path).name
        return f"{file_name}::{chunk_id}"
    
    def _load_existing_vector_store(self) -> Optional[FAISS]:
        """Load existing vector store"""
        vector_store_path = Path(self.vector_store_path)
        if vector_store_path.exists():
            try:
                return FAISS.load_local(
                    str(vector_store_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                print(f"âš ï¸  Existing vector store loading error: {e}")
                return None
        return None
    
    def add_knowledge_file(self, file_path: str) -> bool:
        """Add single file to existing vector store"""
        print(f"ğŸ“„ Adding file: {Path(file_path).name}")
        
        try:
            # Load file contents
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Text splitting
            chunks = self.split_text_into_chunks(content)
            print(f"  ğŸ“ Split into {len(chunks)} chunks")
            
            # Document creation
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": file_path,
                        "chunk_id": i,
                        "total_chunks": len(chunks),
                        "document_id": self._generate_document_id(file_path, i)
                    }
                )
                documents.append(doc)
            
            # Load existing vector store
            vector_store = self._load_existing_vector_store()
            
            if vector_store is None:
                # Create new vector store if it doesn't exist
                print("  ğŸ†• Creating new vector store")
                vector_store = FAISS.from_documents(documents, self.embeddings)
            else:
                # Add to existing vector store
                print("  â• Add to existing vector store")
                vector_store.add_documents(documents)
            
            # Save vector store
            self.save_vector_store(vector_store)
            
            # Update metadata
            metadata = self._load_metadata()
            metadata[file_path] = self._get_file_metadata(file_path)
            self._save_metadata(metadata)
            
            print(f"  âœ… File addition completed: {len(documents)} documents")
            return True
            
        except Exception as e:
            print(f"  âŒ File addition error: {e}")
            return False
    
    def remove_knowledge_file(self, file_path: str) -> bool:
        """Remove single file from existing vector store"""
        print(f"ğŸ—‘ï¸  Deleting file: {Path(file_path).name}")
        
        try:
            # Load existing vector store
            vector_store = self._load_existing_vector_store()
            if vector_store is None:
                print("  âš ï¸  Vector store does not exist")
                return False
            
            # Identify document IDs for deletion
            # Note: FAISS has limited direct deletion functionality,
            # rebuilding the entire store is more reliable in practice
            print("  âš ï¸  Due to FAISS limitations, full rebuild is recommended for file deletion")
            
            # Remove from metadata
            metadata = self._load_metadata()
            if file_path in metadata:
                del metadata[file_path]
                self._save_metadata(metadata)
                print("  âœ… Remove from metadataå®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"  âŒ File deletion error: {e}")
            return False
    
    def update_knowledge_file(self, file_path: str) -> bool:
        """Update single file (deleteâ†’add)"""
        print(f"ğŸ”„ ãƒ•ã‚¡ã‚¤ãƒ«Update: {Path(file_path).name}")
        
        # First delete, then add
        self.remove_knowledge_file(file_path)
        return self.add_knowledge_file(file_path)
    
    def incremental_update(self) -> Dict[str, int]:
        """Update only changed files"""
        print("ğŸ”„ ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«Updateé–‹å§‹...")
        
        # Get current file list
        current_files = self.load_markdown_files()
        stored_metadata = self._load_metadata()
        
        stats = {
            "added": 0,
            "updated": 0,
            "removed": 0,
            "unchanged": 0
        }
        
        # Process new and updated files
        for file_path in current_files:
            if self._file_has_changed(file_path, stored_metadata):
                if file_path in stored_metadata:
                    # Update
                    if self.update_knowledge_file(file_path):
                        stats["updated"] += 1
                else:
                    # New addition
                    if self.add_knowledge_file(file_path):
                        stats["added"] += 1
            else:
                stats["unchanged"] += 1
        
        # Process deleted files
        current_file_set = set(current_files)
        for stored_file in stored_metadata.keys():
            if stored_file not in current_file_set:
                if self.remove_knowledge_file(stored_file):
                    stats["removed"] += 1
        
        print(f"ğŸ“Š ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«Updateå®Œäº†:")
        print(f"  â• Added: {stats['added']} files")
        print(f"  ğŸ”„ Update: {stats['updated']}ãƒ•ã‚¡ã‚¤ãƒ«")  
        print(f"  ğŸ—‘ï¸  Deleted: {stats['removed']} files")
        print(f"  âœ… Unchanged: {stats['unchanged']} files")
        
        return stats


if __name__ == "__main__":
    # Test when run directly
    ingester = KnowledgeIngester()
    documents = ingester.run()
    print(f"Processing completed: {len(documents)} documents") 